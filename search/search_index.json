{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Containerizing your models easily Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built image to predict your data. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Goals Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image Non-goals Some non-goals of this project are: Deploy the built image Getting Started Follow one of our tutorials to easily get started and see how Chassis works: Install with Helm into a Cluster Build and image Deploy to KFServing the built image Contributors A full list of contributors, which includes individuals that have contributed entries, can be found here .","title":"Containerizing your models easily"},{"location":"index.html#containerizing-your-models-easily","text":"Chassis makes it easy to create a deployable docker image from your trained ML model. The idea behind this project is to provide Data Scientists with a way to package their models into a Docker image. This image will manage to build the inference service compatible with several common platforms for free. At the moment, Chassis images are compatible with KFServing and Modzy gRPC. This means you can deploy your built image into these platforms once it has been built. Deploy Chassis, send your model to it and start using the built image to predict your data.","title":"Containerizing your models easily"},{"location":"index.html#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"index.html#goals","text":"Chassis is a Kubernetes service that can be deployed in your preferred cluster using Helm. It works by creating jobs that can be run in parallel to create Docker images that package ML models. It provides integration with most common deployment platforms so your model will be ready to be deployed in a simple way. It also provides a python SDK that makes it very easy to communicate with Chassis service in order to build your image. Simple Just make a request to build your image using the python SDK Small set of dependencies: mlflow, flask Supports multiple deployment platforms No DevOps knowledge needed Fast Start building the image as soon as you make the request Automatically upload the image to Docker Hub Image ready to be deployed Secure Using Kaniko to securely build the image","title":"Goals"},{"location":"index.html#non-goals","text":"Some non-goals of this project are: Deploy the built image","title":"Non-goals"},{"location":"index.html#getting-started","text":"Follow one of our tutorials to easily get started and see how Chassis works: Install with Helm into a Cluster Build and image Deploy to KFServing the built image","title":"Getting Started"},{"location":"index.html#contributors","text":"A full list of contributors, which includes individuals that have contributed entries, can be found here .","title":"Contributors"},{"location":"faqs.html","text":"FAQs Does boxkite support other models besides MLFlow? Not at the moment. MLFlow is a way that allows Chassis to manage all kind of original models. So you can load your SKLearn, PyTorch, TF... model and convert it to MLFlow before sending it to Chassis.","title":"FAQs"},{"location":"faqs.html#faqs","text":"","title":"FAQs"},{"location":"faqs.html#does-boxkite-support-other-models-besides-mlflow","text":"Not at the moment. MLFlow is a way that allows Chassis to manage all kind of original models. So you can load your SKLearn, PyTorch, TF... model and convert it to MLFlow before sending it to Chassis.","title":"Does boxkite support other models besides MLFlow?"},{"location":"get-involved.html","text":"Get Involved Feel free to fork and open pull requests to contribute to modzy/chassis . Join our #chassis-model-builder Slack channel on the MLOps.community Slack to chat to us!","title":"Get Involved"},{"location":"get-involved.html#get-involved","text":"Feel free to fork and open pull requests to contribute to modzy/chassis . Join our #chassis-model-builder Slack channel on the MLOps.community Slack to chat to us!","title":"Get Involved"},{"location":"help.html","text":"Help Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you!","title":"Help"},{"location":"help.html#help","text":"Join our #chassis-model-builder Slack channel on the MLOps.community Slack and we'll gladly try to help you!","title":"Help"},{"location":"release-notes.html","text":"Release Notes See also: PyPI . v0.0.1 First working release of Chassis.","title":"Release Notes"},{"location":"release-notes.html#release-notes","text":"See also: PyPI .","title":"Release Notes"},{"location":"release-notes.html#v001","text":"First working release of Chassis.","title":"v0.0.1"},{"location":"tutorials/devops-deploy.html","text":"How a DevOps person would deploy Chassis Add the Helm repository helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update Install Chassis service Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis Check the installation After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... ) Query the service To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"How a DevOps person would deploy Chassis"},{"location":"tutorials/devops-deploy.html#how-a-devops-person-would-deploy-chassis","text":"","title":"How a DevOps person would deploy Chassis"},{"location":"tutorials/devops-deploy.html#add-the-helm-repository","text":"helm repo add chassis https://modzy.github.io/chassis After that we just need to update the Helm repos to fetch Chassis data. helm repo update","title":"Add the Helm repository"},{"location":"tutorials/devops-deploy.html#install-chassis-service","text":"Now we just need to install Chassis as normal using Helm. helm install chassis chassis/chassis","title":"Install Chassis service"},{"location":"tutorials/devops-deploy.html#check-the-installation","text":"After having installed the service we can check that the Chassis service is correctly deployed. kubectl get svc/chassis Then you should see an output similar to this. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE chassis NodePort 10 .106.209.207 <none> 5000 :30496/TCP 15s We can also check that the pod that runs the service is correctly running. kubectl get pods Where we should find our pod listed. NAME READY STATUS RESTARTS AGE ( ... ) chassis-5c574d459c-rclx9 1 /1 Running 0 22s ( ... )","title":"Check the installation"},{"location":"tutorials/devops-deploy.html#query-the-service","text":"To conclude, we may want to query the service just to see that it answers as we expect. To do that, we need to port forward the service. kubectl port-forward svc/chassis 5000 :5000 Now that we have access to the service we can query it. curl localhost:5000 Which should output an alive message.","title":"Query the service"},{"location":"tutorials/ds-connect.html","text":"How a Data Scientist connects to chassis and builds a model In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it. Install the SDK First step is to install the SDK using pip . pip install chassisml Build or import the model We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it. Import required libraries Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn import mlflow.pyfunc from joblib import dump , load Create the model Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) dump ( clf , './model.joblib' ) Transform the model to MLFlow Once that we have our model we transform it to MLFlow format. class CustomModel ( mlflow . pyfunc . PythonModel ): _model = load ( './model.joblib' ) def load_context ( self , context ): self . model = self . _model def predict ( self , context , inputs ): processed_inputs = self . pre_process ( inputs ) inference_results = self . model . predict ( processed_inputs ) return self . post_process ( inference_results ) def pre_process ( self , inputs ): return inputs / 2 def post_process ( self , inference_results ): structured_results = [] for inference_result in inference_results : inference_result = { 'classPredictions' : [ { 'class' : str ( inference_result ), 'score' : str ( 1 )} ] } structured_output = { 'data' : { 'result' : inference_result , 'explanation' : None , 'drift' : None , } } structured_results . append ( structured_output ) return structured_results Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. This way, in the load_context function we just need to point to the model that is already loaded in memory. All other functions like predict , pre_process or post_process are completely up to you and the requirements of your model. This is the conda environment that we are going to define in this case for our model. conda_env = { 'channels' : [ 'defaults' , 'conda-forge' , 'pytorch' ], 'dependencies' : [ 'python=3.8.5' , 'pytorch' , 'torchvision' , 'pip' , { 'pip' : [ 'mlflow' , 'lime' , 'sklearn' ], }, ], 'name' : 'linear_env' } Finally we just save the MLFlow model in the directory we prefer. model_save_path = 'mlflow_custom_pyfunc_svm' mlflow . pyfunc . save_model ( path = model_save_path , python_model = CustomModel (), conda_env = conda_env ) Build the image Now that we have our model in MLFlow format we need to make a request against the Chassis service to build the Docker image that exposes it. Define data There is some model and image related data that we need to define before we send our model to Chassis . In case we want Chassis to upload our image to Docker Hub we can pass the credentials in base64 format. echo -n \"<user>:<password>\" | base64 Which we are going to asume that outputs XxXxXxXx . This is an example of the data that we could use. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' } As we can see, we must define the following fields: name : tag of the image model_name : name of the model we trained before model_path : the directory where we have stored our MLFlow model registry_path : credentials in case we want to upload the image Make the request Now we can make the request to let Chassis build our image by making a request. We can decide if we want Chassis to upload the image to Docker Hub and we can also modify the address of the service. res = chassisml . publish ( image_data = image_data , deploy = True , # True if we want Chassis to upload the image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d Check the job status As we have seen, we have been assigned to an uid that we can use to check the status of the job that is building our image. chassisml . get_job_status ( job_id ) And we should see something like this in case the job has not finished yet. { 'ac t ive' : 1 , 'comple t io n _ t ime' : No ne , 'co n di t io ns ' : No ne , ' fa iled' : No ne , 's tart _ t ime' : 'Fri , 09 Jul 2021 09 : 01 : 43 GMT' , 'succeeded' : No ne } On the other hand, if the job has already finished and our image has been correctly built this could be the output. { 'ac t ive' : No ne , 'comple t io n _ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'co n di t io ns ' : [{ 'las t _probe_ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'las t _ trans i t io n _ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'message' : No ne , 'reaso n ' : No ne , 's tatus ' : 'True' , ' t ype' : 'Comple te ' }], ' fa iled' : No ne , 's tart _ t ime' : 'Fri , 09 Jul 2021 09 : 01 : 43 GMT' , 'succeeded' : 1 } Pull the image Now that the process has completely finished we can pull and see our built image. docker pull <user>/chassisml-sklearn-demo:latest docker images <user>/chassisml-sklearn-demo:latest If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/chassisml-sklearn-demo latest 0e5c5815f2ec 3 minutes ago 2 .19GB","title":"How a Data Scientist connects to chassis and builds a model"},{"location":"tutorials/ds-connect.html#how-a-data-scientist-connects-to-chassis-and-builds-a-model","text":"In order to connect to Chassis service we are going to use the SDK. We will transform our model into MLFlow format and we will upload it by making a request. After that, the image that have been created will be uploaded to Docker Hub and we will be able to use it.","title":"How a Data Scientist connects to chassis and builds a model"},{"location":"tutorials/ds-connect.html#install-the-sdk","text":"First step is to install the SDK using pip . pip install chassisml","title":"Install the SDK"},{"location":"tutorials/ds-connect.html#build-or-import-the-model","text":"We can start from an existing model or create a new one. After that, we will need to transform it to MLFlow format so Chassis service will be able to manage it.","title":"Build or import the model"},{"location":"tutorials/ds-connect.html#import-required-libraries","text":"Since we are going to train our own model as an example, we need to import all the libraries that we will need to do that. import chassisml import sklearn import mlflow.pyfunc from joblib import dump , load","title":"Import required libraries"},{"location":"tutorials/ds-connect.html#create-the-model","text":"Just as an example we are going to create and train a simple SKLearn model. from sklearn import datasets , svm from sklearn.model_selection import train_test_split digits = datasets . load_digits () data = digits . images . reshape (( len ( digits . images ), - 1 )) # Create a classifier: a support vector classifier clf = svm . SVC ( gamma = 0.001 ) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) # Learn the digits on the train subset clf . fit ( X_train , y_train ) dump ( clf , './model.joblib' )","title":"Create the model"},{"location":"tutorials/ds-connect.html#transform-the-model-to-mlflow","text":"Once that we have our model we transform it to MLFlow format. class CustomModel ( mlflow . pyfunc . PythonModel ): _model = load ( './model.joblib' ) def load_context ( self , context ): self . model = self . _model def predict ( self , context , inputs ): processed_inputs = self . pre_process ( inputs ) inference_results = self . model . predict ( processed_inputs ) return self . post_process ( inference_results ) def pre_process ( self , inputs ): return inputs / 2 def post_process ( self , inference_results ): structured_results = [] for inference_result in inference_results : inference_result = { 'classPredictions' : [ { 'class' : str ( inference_result ), 'score' : str ( 1 )} ] } structured_output = { 'data' : { 'result' : inference_result , 'explanation' : None , 'drift' : None , } } structured_results . append ( structured_output ) return structured_results Notice that the SKLearn model that we created before is loaded into memory so that it will be packaged inside the MLFlow model. This way, in the load_context function we just need to point to the model that is already loaded in memory. All other functions like predict , pre_process or post_process are completely up to you and the requirements of your model. This is the conda environment that we are going to define in this case for our model. conda_env = { 'channels' : [ 'defaults' , 'conda-forge' , 'pytorch' ], 'dependencies' : [ 'python=3.8.5' , 'pytorch' , 'torchvision' , 'pip' , { 'pip' : [ 'mlflow' , 'lime' , 'sklearn' ], }, ], 'name' : 'linear_env' } Finally we just save the MLFlow model in the directory we prefer. model_save_path = 'mlflow_custom_pyfunc_svm' mlflow . pyfunc . save_model ( path = model_save_path , python_model = CustomModel (), conda_env = conda_env )","title":"Transform the model to MLFlow"},{"location":"tutorials/ds-connect.html#build-the-image","text":"Now that we have our model in MLFlow format we need to make a request against the Chassis service to build the Docker image that exposes it.","title":"Build the image"},{"location":"tutorials/ds-connect.html#define-data","text":"There is some model and image related data that we need to define before we send our model to Chassis . In case we want Chassis to upload our image to Docker Hub we can pass the credentials in base64 format. echo -n \"<user>:<password>\" | base64 Which we are going to asume that outputs XxXxXxXx . This is an example of the data that we could use. image_data = { 'name' : '<user>/chassisml-sklearn-demo:latest' , 'model_name' : 'digits' , 'model_path' : './mlflow_custom_pyfunc_svm' , 'registry_auth' : 'XxXxXxXx' } As we can see, we must define the following fields: name : tag of the image model_name : name of the model we trained before model_path : the directory where we have stored our MLFlow model registry_path : credentials in case we want to upload the image","title":"Define data"},{"location":"tutorials/ds-connect.html#make-the-request","text":"Now we can make the request to let Chassis build our image by making a request. We can decide if we want Chassis to upload the image to Docker Hub and we can also modify the address of the service. res = chassisml . publish ( image_data = image_data , deploy = True , # True if we want Chassis to upload the image base_url = 'http://localhost:5000' ) error = res . get ( 'error' ) job_id = res . get ( 'job_id' ) if error : print ( 'Error:' , error ) else : print ( 'Job ID:' , job_id ) If everything has gone well we should see something similar to this. Publishing container... Ok! Job ID: chassis-builder-job-a3864869-a509-4658-986b-25cb4ddd604d","title":"Make the request"},{"location":"tutorials/ds-connect.html#check-the-job-status","text":"As we have seen, we have been assigned to an uid that we can use to check the status of the job that is building our image. chassisml . get_job_status ( job_id ) And we should see something like this in case the job has not finished yet. { 'ac t ive' : 1 , 'comple t io n _ t ime' : No ne , 'co n di t io ns ' : No ne , ' fa iled' : No ne , 's tart _ t ime' : 'Fri , 09 Jul 2021 09 : 01 : 43 GMT' , 'succeeded' : No ne } On the other hand, if the job has already finished and our image has been correctly built this could be the output. { 'ac t ive' : No ne , 'comple t io n _ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'co n di t io ns ' : [{ 'las t _probe_ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'las t _ trans i t io n _ t ime' : 'Fri , 09 Jul 2021 09 : 13 : 37 GMT' , 'message' : No ne , 'reaso n ' : No ne , 's tatus ' : 'True' , ' t ype' : 'Comple te ' }], ' fa iled' : No ne , 's tart _ t ime' : 'Fri , 09 Jul 2021 09 : 01 : 43 GMT' , 'succeeded' : 1 }","title":"Check the job status"},{"location":"tutorials/ds-connect.html#pull-the-image","text":"Now that the process has completely finished we can pull and see our built image. docker pull <user>/chassisml-sklearn-demo:latest docker images <user>/chassisml-sklearn-demo:latest If everything has gone as expected we will see something similar to this. REPOSITORY TAG IMAGE ID CREATED SIZE <user>/chassisml-sklearn-demo latest 0e5c5815f2ec 3 minutes ago 2 .19GB","title":"Pull the image"},{"location":"tutorials/ds-deploy.html","text":"How a Data Scientist deploys to KFServing This is assuming that you have already downloaded your containerized tar image and it is stored in your disk as /tmp/downloaded_image.tar . Install KFServing in minikube You just need to clone the KFServing repository and run the quick_install.sh script. git clone git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh Required variables There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined Deploy the model Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message. Define required variables to query the pod This is needed in order to be able to communicate with the deployed image. The MODEL_NAME must match the name defined in the InferenceService yaml file . export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 ) Query the model Now you can just make a request to predict some data: curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 . Deploy the model locally The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way: curl localhost:8080/v2/models/digits/infer -d@/tmp/inputsv2.json","title":"How a Data Scientist deploys to KFServing"},{"location":"tutorials/ds-deploy.html#how-a-data-scientist-deploys-to-kfserving","text":"This is assuming that you have already downloaded your containerized tar image and it is stored in your disk as /tmp/downloaded_image.tar .","title":"How a Data Scientist deploys to KFServing"},{"location":"tutorials/ds-deploy.html#install-kfserving-in-minikube","text":"You just need to clone the KFServing repository and run the quick_install.sh script. git clone git@github.com:kubeflow/kfserving.git ./kfserving/hack/quick_install.sh","title":"Install KFServing in minikube"},{"location":"tutorials/ds-deploy.html#required-variables","text":"There are some environment variables that must be defined for KFServing to work: INTERFACE: kfserving HTTP_PORT: port where kfserving will be running PROTOCOL: it can be v1 or v2 MODEL_NAME: a name for the model must be defined","title":"Required variables"},{"location":"tutorials/ds-deploy.html#deploy-the-model","text":"Assuming the image generated by ChassisML has been uploaded to a repository called carmilso/chassisml-sklearn-demo:latest , just deploy the file that defines the InferenceService for the protocol v1 of KFServing apiVersion : \"serving.kubeflow.org/v1beta1\" kind : \"InferenceService\" metadata : name : chassisml-sklearn-demo spec : predictor : containers : - image : carmilso/chassisml-sklearn-demo:latest name : chassisml-sklearn-demo-container env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits ports : - containerPort : 8080 protocol : TCP In this case, the variable MODEL_NAME should not be necessary since it's defined when creating the image. kubectl apply -f custom_v1.yaml This should output a success message.","title":"Deploy the model"},{"location":"tutorials/ds-deploy.html#define-required-variables-to-query-the-pod","text":"This is needed in order to be able to communicate with the deployed image. The MODEL_NAME must match the name defined in the InferenceService yaml file . export INGRESS_HOST = $( minikube ip ) export INGRESS_PORT = $( kubectl -n istio-system get service istio-ingressgateway -o jsonpath = '{.spec.ports[?(@.name==\"http2\")].nodePort}' ) export MODEL_NAME = digits export SERVICE_HOSTNAME = $( kubectl get inferenceservice ${ MODEL_NAME } -o jsonpath = '{.status.url}' | cut -d \"/\" -f 3 )","title":"Define required variables to query the pod"},{"location":"tutorials/ds-deploy.html#query-the-model","text":"Now you can just make a request to predict some data: curl -H \"Host: ${ SERVICE_HOSTNAME } \" \"http:// ${ INGRESS_HOST } : ${ INGRESS_PORT } /v1/models/ ${ MODEL_NAME } :predict\" -d@inputsv1.json | jq The output should be similar to this: { \"predictions\" : [ { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"4\" , \"score\" : \"1\" } ] } } }, { \"data\" : { \"drift\" : null , \"explanation\" : null , \"result\" : { \"classPredictions\" : [ { \"class\" : \"8\" , \"score\" : \"1\" } ] } } } ] } In this case, the data was prepared for the protocol v1, but we can deploy the image using the protocol v2 and make the request using the data for v2 .","title":"Query the model"},{"location":"tutorials/ds-deploy.html#deploy-the-model-locally","text":"The model can also be deployed locally: docker run --rm -p 8080 :8080 \\ -e INTERFACE = kfserving \\ -e HTTP_PORT = 8080 \\ -e PROTOCOL = v2 \\ carmilso/chassisml-sklearn-demo:latest So we can query it this way: curl localhost:8080/v2/models/digits/infer -d@/tmp/inputsv2.json","title":"Deploy the model locally"}]}